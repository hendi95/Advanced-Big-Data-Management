{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea206360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IST769 Midterm\n",
    "#-----------------\n",
    "\n",
    "# You will turn this file in along with screenshots as instructed on Blackboard\n",
    "\n",
    "# YOUR NAME: Hendi Kushta\n",
    "# YOUR EMAIL: hkushta@syr.edu\n",
    "# YOUR SUID: 7522109662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4879e4b4-0260-40c5-bc36-69f1b6ac5630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "com.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n",
      "com.microsoft.sqlserver#mssql-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d409417d-8ba1-46fe-8f4d-963dc3b79732;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.1.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.2.0 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;12.2.0.jre11 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.1.2/hadoop-aws-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.1.2!hadoop-aws.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/spark-mssql-connector_2.12/1.2.0/spark-mssql-connector_2.12-1.2.0.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#spark-mssql-connector_2.12;1.2.0!spark-mssql-connector_2.12.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/12.2.0.jre11/mssql-jdbc-12.2.0.jre11.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.sqlserver#mssql-jdbc;12.2.0.jre11!mssql-jdbc.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.271/aws-java-sdk-bundle-1.11.271.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.271!aws-java-sdk-bundle.jar (1961ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (18ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (46ms)\n",
      ":: resolution report :: resolve 3977ms :: artifacts dl 2225ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n",
      "\tcom.microsoft.azure#spark-mssql-connector_2.12;1.2.0 from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;12.2.0.jre11 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.1.2 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   8   |   8   |   0   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d409417d-8ba1-46fe-8f4d-963dc3b79732\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (88869kB/126ms)\n",
      "24/03/08 06:35:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "user = \"minio\"\n",
    "passwd = \"SU2orange!\"\n",
    "s3_bucket = \"gamestreams\"\n",
    "s3_server = \"http://minio:9000\"\n",
    "s3_access_key = user\n",
    "s3_secret_key = passwd\n",
    "mongo_user = \"mongo\"\n",
    "mongo_uri = f\"mongodb://{mongo_user}:{passwd}@mongo:27017/admin?authSource=admin\"\n",
    "server_name = \"jdbc:sqlserver://mssql\"\n",
    "database_name = \"sidearmdb\"\n",
    "mssql_user = \"sa\"\n",
    "mssql_pw = passwd\n",
    "mssql_url = server_name + \";\" + \"databaseName=\" + database_name + \";encrypt=true;trustServerCertificate=true;\"\n",
    "\n",
    "jars = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.1.2\",\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n",
    "    \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\",\n",
    "    \"com.microsoft.sqlserver:mssql-jdbc:12.2.0.jre11\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"spark.jars.packages\",\",\".join(jars) )\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", s3_server ) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d93a740-0b7c-410f-a8ed-2ad5734c34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+------+\n",
      "| id|  name|number|shots|goals|teamid|\n",
      "+---+------+------+-----+-----+------+\n",
      "|  1|   sam|     6|   56|   23|   101|\n",
      "|  2| sarah|     1|   85|   34|   101|\n",
      "|  3| steve|     2|   60|   20|   101|\n",
      "|  4| stone|    13|   33|   10|   101|\n",
      "|  5|  sean|    17|   26|    9|   101|\n",
      "|  6|   sly|     8|   78|   15|   101|\n",
      "|  7|   sol|     9|   52|   20|   101|\n",
      "|  8| shree|     4|   20|    4|   101|\n",
      "|  9|shelly|    15|   10|    2|   101|\n",
      "| 10| swede|    10|   90|   50|   101|\n",
      "| 11| jimmy|     1|  100|   50|   205|\n",
      "| 12| julie|     9|   10|    0|   205|\n",
      "| 13| james|     2|   45|   15|   205|\n",
      "| 14|  jane|    15|   82|   46|   205|\n",
      "| 15| jimmy|    16|   42|   30|   205|\n",
      "| 16| julie|     8|   67|   32|   205|\n",
      "| 17| james|    17|   40|   14|   205|\n",
      "| 18|  jane|     3|   91|   40|   205|\n",
      "| 19| jimmy|     5|   78|   22|   205|\n",
      "| 20| julie|    22|   83|   19|   205|\n",
      "+---+------+------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HOW TO READ FROM MSSQL\n",
    "df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2486f5fd-62fe-41c7-b042-53098cb6da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO WRITE TO MSSQL\n",
    "df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players2\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60bb340d-d6db-419e-9eb2-55f53f079cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+--------------------+-------------+\n",
      "| id|event_time|team_id|player_jersey_number|scored_or_not|\n",
      "+---+----------+-------+--------------------+-------------+\n",
      "|  0|     59:51|    101|                   2|            0|\n",
      "|  1|     57:06|    101|                   6|            0|\n",
      "|  2|     56:13|    205|                   8|            1|\n",
      "|  3|     55:25|    101|                   4|            0|\n",
      "|  4|     55:03|    101|                   1|            1|\n",
      "+---+----------+-------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Read the gamestream.txt from minio\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "gamestream = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .text(\"s3a://gamestreams/gamestream.txt\") \\\n",
    "    .withColumn('value_split', split('value', ' ')) \\\n",
    "    .selectExpr(\"value_split[0] as id\",\n",
    "                \"value_split[1] as event_time\",\n",
    "                \"value_split[2] as team_id\",\n",
    "                \"value_split[3] as player_jersey_number\",\n",
    "                \"value_split[4] as scored_or_not\")\n",
    "\n",
    "\n",
    "gamestream.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb788d2f-d58f-4d42-bc40-124facbb8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write the gamestream to mongodb\n",
    "\n",
    "gamestream.write \\\n",
    "    .format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", \"gamestreams\") \\\n",
    "    .option(\"collection\", \"gamestream\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3565b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS TO EXAM QUESTIONS\n",
    "# PLACE YOUR ANSWERS BELOW TURN THIS .ipynb FILE IN ALONG WITH SCREENSHOTS AS INSTRUCTED ON BLACKBOARD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc97c1-e4be-4acd-82b9-8a59534313f8",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ec71725-e206-467c-9c6e-742446210560",
   "metadata": {},
   "source": [
    "# Q1\n",
    "SELECT\n",
    "    t.name AS team_name,\n",
    "    t.wins AS team_wins,\n",
    "    t.losses AS team_losses,\n",
    "    p.name AS player_name,\n",
    "    p.shots AS player_shots,\n",
    "    p.goals AS player_goals\n",
    "FROM\n",
    "    teams t\n",
    "JOIN\n",
    "    players p ON t.id = p.teamid;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856864f2-ecb6-4156-a759-0e255d3af7af",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1548cb84-a8fe-42f3-b0ca-a75d095e031a",
   "metadata": {},
   "source": [
    "# Q2\n",
    "SELECT\n",
    "  CAST(columns[0] AS INT) AS event_id,\n",
    "  columns[1] AS event_timestamp,\n",
    "  CAST(columns[2] AS INT) AS team_id,\n",
    "  CAST(columns[3] AS INT) AS player_jersey_number,\n",
    "  CAST(columns[4] AS INT) AS shot_result\n",
    "FROM\n",
    "  minio.`gamestream.txt`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f69a1-d10d-4c71-af1b-f56bb1b2d0b4",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cf344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------+--------------------+-----------+\n",
      "|event_id|event_timestamp|team_id|player_jersey_number|shot_result|\n",
      "+--------+---------------+-------+--------------------+-----------+\n",
      "|       0|          59:51|    101|                   2|          0|\n",
      "|       1|          57:06|    101|                   6|          0|\n",
      "|       2|          56:13|    205|                   8|          1|\n",
      "|       3|          55:25|    101|                   4|          0|\n",
      "|       4|          55:03|    101|                   1|          1|\n",
      "+--------+---------------+-------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "gamestream = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .text(\"s3a://gamestreams/gamestream.txt\") \\\n",
    "    .withColumn('columns', split('value', ' ')) \\\n",
    "    .selectExpr(\"columns[0] as event_id\",\n",
    "                \"columns[1] as event_timestamp\",\n",
    "                \"columns[2] as team_id\",\n",
    "                \"columns[3] as player_jersey_number\",\n",
    "                \"columns[4] as shot_result\")\n",
    "\n",
    "gamestream.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af5679-b19f-474f-859c-3880b1fbad3e",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d4e28f-9bf6-4c61-91f0-fe03bb0f8990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=========================================>             (150 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-----------+----------------+\n",
      "|team_id|player_jersey_number|total_shots|total_goals|total_team_goals|\n",
      "+-------+--------------------+-----------+-----------+----------------+\n",
      "|    101|                   8|          4|          0|            14.0|\n",
      "|    101|                   1|          8|          6|            14.0|\n",
      "|    101|                  15|          3|          1|            14.0|\n",
      "|    101|                   2|          7|          2|            14.0|\n",
      "|    101|                   6|          4|          2|            14.0|\n",
      "|    101|                   4|          5|          1|            14.0|\n",
      "|    101|                   9|          5|          0|            14.0|\n",
      "|    101|                  10|          3|          1|            14.0|\n",
      "|    101|                  17|          2|          0|            14.0|\n",
      "|    101|                  13|          7|          1|            14.0|\n",
      "|    205|                   2|          3|          1|             9.0|\n",
      "|    205|                  17|          3|          1|             9.0|\n",
      "|    205|                  15|          2|          2|             9.0|\n",
      "|    205|                   9|          4|          0|             9.0|\n",
      "|    205|                   1|          3|          3|             9.0|\n",
      "|    205|                  22|          1|          0|             9.0|\n",
      "|    205|                   8|          2|          1|             9.0|\n",
      "|    205|                  16|          1|          0|             9.0|\n",
      "|    205|                   5|          2|          1|             9.0|\n",
      "|    205|                   3|          1|          0|             9.0|\n",
      "+-------+--------------------+-----------+-----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "from pyspark.sql.functions import countDistinct, sum, when\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df_gamestream = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .text(\"s3a://gamestreams/gamestream.txt\") \\\n",
    "    .withColumn('columns', split('value', ' ')) \\\n",
    "    .selectExpr(\"columns[0] as event_id\",\n",
    "                \"columns[1] as event_timestamp\",\n",
    "                \"columns[2] as team_id\",\n",
    "                \"columns[3] as player_jersey_number\",\n",
    "                \"columns[4] as shot_result\")\n",
    "\n",
    "# Drop records where team_id = 0\n",
    "df_gamestream = df_gamestream.filter(df_gamestream[\"team_id\"] != 0)\n",
    "\n",
    "# Group by team_id and calculate total goals for the team\n",
    "team_stats = df_gamestream.groupBy(\"team_id\") \\\n",
    "    .agg(\n",
    "        sum(\"shot_result\").alias(\"total_team_goals\")\n",
    "    )\n",
    "\n",
    "# Group by team_id and player_jersey_number and calculate total shots for each player\n",
    "player_stats = df_gamestream.groupBy(\"team_id\", \"player_jersey_number\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"event_id\").alias(\"total_shots\"),\n",
    "        sum(when(df_gamestream[\"shot_result\"] == 1, 1).otherwise(0)).alias(\"total_goals\")\n",
    "    )\n",
    "\n",
    "# Joining both dataframes on team_id\n",
    "final_df = player_stats.join(team_stats, \"team_id\", \"inner\")\n",
    "\n",
    "# Order by team_id and player_id\n",
    "final_df = final_df.orderBy(\"team_id\")\n",
    "\n",
    "# Show the final dataframe\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86299a3-2628-4ff2-9e58-cf17b75848a8",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5816ed8-5f18-48a5-a1ba-33308dea697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================================>        (167 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------+--------------------+-----------+-----------+----------------+\n",
      "|event_id|event_timestamp|team_id|player_jersey_number|total_shots|total_goals|total_team_goals|\n",
      "+--------+---------------+-------+--------------------+-----------+-----------+----------------+\n",
      "|      70|          00:00|    101|                   8|          4|          0|              14|\n",
      "|      70|          00:00|    101|                   1|          8|          6|              14|\n",
      "|      70|          00:00|    101|                  15|          3|          1|              14|\n",
      "|      70|          00:00|    101|                   2|          7|          2|              14|\n",
      "|      70|          00:00|    101|                   6|          4|          2|              14|\n",
      "|      70|          00:00|    101|                   4|          5|          1|              14|\n",
      "|      70|          00:00|    101|                   9|          5|          0|              14|\n",
      "|      70|          00:00|    101|                  10|          3|          1|              14|\n",
      "|      70|          00:00|    101|                  17|          2|          0|              14|\n",
      "|      70|          00:00|    101|                  13|          7|          1|              14|\n",
      "|      70|          00:00|    205|                   2|          3|          1|               9|\n",
      "|      70|          00:00|    205|                  17|          3|          1|               9|\n",
      "|      70|          00:00|    205|                  15|          2|          2|               9|\n",
      "|      70|          00:00|    205|                   9|          4|          0|               9|\n",
      "|      70|          00:00|    205|                   1|          3|          3|               9|\n",
      "|      70|          00:00|    205|                  22|          1|          0|               9|\n",
      "|      70|          00:00|    205|                   8|          2|          1|               9|\n",
      "|      70|          00:00|    205|                  16|          1|          0|               9|\n",
      "|      70|          00:00|    205|                   5|          2|          1|               9|\n",
      "|      70|          00:00|    205|                   3|          1|          0|               9|\n",
      "+--------+---------------+-------+--------------------+-----------+-----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "\n",
    "from pyspark.sql.functions import countDistinct, sum, when, split, lit, col\n",
    "\n",
    "gamestream = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .text(\"s3a://gamestreams/gamestream.txt\") \\\n",
    "    .withColumn('columns', split('value', ' ')) \\\n",
    "    .selectExpr(\"columns[0] as event_id\",\n",
    "                \"columns[1] as event_timestamp\",\n",
    "                \"columns[2] as team_id\",\n",
    "                \"columns[3] as player_jersey_number\",\n",
    "                \"columns[4] as shot_result\")\n",
    "\n",
    "# Cast event_id column to integers\n",
    "gamestream = gamestream.withColumn(\"event_id\", col(\"event_id\").cast(\"int\"))\n",
    "\n",
    "# Calculate the maximum event_id and corresponding event_timestamp\n",
    "max_event_id_info = gamestream.orderBy(col(\"event_id\").desc()).select(\"event_id\", \"event_timestamp\").limit(1)\n",
    "\n",
    "# Aggregations\n",
    "output = gamestream \\\n",
    "    .groupBy(\"team_id\", \"player_jersey_number\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"event_id\").alias(\"total_shots\"),\n",
    "        sum(when(gamestream[\"shot_result\"] == 1, 1).otherwise(0)).alias(\"total_goals\")\n",
    "    )\n",
    "\n",
    "# Calculate total team goals\n",
    "team_stats = gamestream.groupBy(\"team_id\") \\\n",
    "    .agg(\n",
    "        sum(when(gamestream[\"shot_result\"] == 1, 1).otherwise(0)).alias(\"total_team_goals\")\n",
    "    )\n",
    "\n",
    "# Join with max_event_id_info to add event_id and timestamp columns\n",
    "output = output.join(team_stats, \"team_id\", \"inner\").crossJoin(max_event_id_info)\n",
    "\n",
    "# Rearrange columns and rename them\n",
    "output = output.select(\"event_id\", \"event_timestamp\", \"team_id\", \"player_jersey_number\",\n",
    "                       \"total_shots\", \"total_goals\", \"total_team_goals\")\n",
    "\n",
    "# Drop records where team_id = 0\n",
    "output = output.filter(output[\"team_id\"] != 0)\n",
    "\n",
    "# Order by team_id\n",
    "output = output.orderBy(\"team_id\")\n",
    "\n",
    "# Show the result\n",
    "output.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7f342-00df-42ec-a455-1a3fba19c859",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c588255a-f683-476f-84c1-56dd332df7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+--------------------+-----------+-----------+----------------+---------+-----------+-------------+---------------+---------+-----------+\n",
      "|team_id|event_id|event_timestamp|player_jersey_number|total_shots|total_goals|total_team_goals|player_id|player_name|    team_name|team_conference|team_wins|team_losses|\n",
      "+-------+--------+---------------+--------------------+-----------+-----------+----------------+---------+-----------+-------------+---------------+---------+-----------+\n",
      "|    101|      70|          00:00|                   6|          4|          2|              14|        1|        sam|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                   1|          8|          6|              14|        2|      sarah|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                   2|          7|          2|              14|        3|      steve|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                  13|          7|          1|              14|        4|      stone|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                  17|          2|          0|              14|        5|       sean|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                   8|          4|          0|              14|        6|        sly|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                   9|          5|          0|              14|        7|        sol|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                   4|          5|          1|              14|        8|      shree|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                  15|          3|          1|              14|        9|     shelly|     syracuse|            acc|       11|          2|\n",
      "|    101|      70|          00:00|                  10|          3|          1|              14|       10|      swede|     syracuse|            acc|       11|          2|\n",
      "|    205|      70|          00:00|                   1|          3|          3|               9|       11|      jimmy|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                   9|          4|          0|               9|       12|      julie|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                   2|          3|          1|               9|       13|      james|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                  15|          2|          2|               9|       14|       jane|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                  16|          1|          0|               9|       15|      jimmy|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                   8|          2|          1|               9|       16|      julie|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                  17|          3|          1|               9|       17|      james|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                   3|          1|          0|               9|       18|       jane|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                   5|          2|          1|               9|       19|      jimmy|johns hopkins|          big10|        9|          4|\n",
      "|    205|      70|          00:00|                  22|          1|          0|               9|       20|      julie|johns hopkins|          big10|        9|          4|\n",
      "+-------+--------+---------------+--------------------+-----------+-----------+----------------+---------+-----------+-------------+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read gamestream data\n",
    "gamestream = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .text(\"s3a://gamestreams/gamestream.txt\") \\\n",
    "    .withColumn('columns', F.split('value', ' ')) \\\n",
    "    .selectExpr(\"columns[0] as event_id\",\n",
    "                \"columns[1] as event_timestamp\",\n",
    "                \"columns[2] as team_id\",\n",
    "                \"columns[3] as player_jersey_number\",\n",
    "                \"columns[4] as shot_result\")\n",
    "\n",
    "# Cast event_id column to integers\n",
    "gamestream = gamestream.withColumn(\"event_id\", F.col(\"event_id\").cast(\"int\"))\n",
    "\n",
    "# Calculate the maximum event_id and corresponding event_timestamp\n",
    "max_event_info = gamestream.selectExpr(\n",
    "    \"max(event_id) as max_event_id\",\n",
    "    \"min(event_timestamp) as min_event_timestamp\"\n",
    ").head()\n",
    "\n",
    "# Aggregations\n",
    "output = gamestream \\\n",
    "    .groupBy(\"team_id\", \"player_jersey_number\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(\"event_id\").alias(\"total_shots\"),\n",
    "        F.sum(F.when(gamestream[\"shot_result\"] == 1, 1).otherwise(0)).alias(\"total_goals\"),\n",
    "        F.min(F.col(\"event_timestamp\")).alias(\"min_event_timestamp\")\n",
    "    )\n",
    "\n",
    "# Fill null values with 0 for total_team_goals\n",
    "output = output.fillna(0, subset=[\"total_goals\"])\n",
    "\n",
    "# Calculate total team goals\n",
    "team_stats = gamestream.groupBy(\"team_id\") \\\n",
    "    .agg(\n",
    "        F.sum(F.when(gamestream[\"shot_result\"] == 1, 1).otherwise(0)).alias(\"total_team_goals\")\n",
    "    )\n",
    "\n",
    "# Join with max_event_info to add event_id and timestamp columns\n",
    "output = output.join(team_stats, \"team_id\", \"inner\")\n",
    "\n",
    "# Add the maximum event ID and timestamp\n",
    "output = output.withColumn(\"max_event_id\", F.lit(max_event_info[\"max_event_id\"])) \\\n",
    "    .withColumn(\"min_event_timestamp\", F.lit(max_event_info[\"min_event_timestamp\"]))\n",
    "\n",
    "# Rearrange columns and rename them\n",
    "output = output.select(\"max_event_id\", \"min_event_timestamp\", \"team_id\", \"player_jersey_number\",\n",
    "                       \"total_shots\", \"total_goals\", \"total_team_goals\")\n",
    "\n",
    "# Read player reference data from MSSQL\n",
    "players_df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "# Read team reference data from MSSQL\n",
    "teams_df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"teams\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "# Join players and teams\n",
    "players_and_teams = players_df.join(teams_df, players_df[\"teamid\"] == teams_df[\"id\"], \"left\") \\\n",
    "    .select(players_df[\"*\"], teams_df[\"name\"].alias(\"team_name\"),\n",
    "            teams_df[\"conference\"].alias(\"team_conference\"),\n",
    "            teams_df[\"wins\"].alias(\"team_wins\"),\n",
    "            teams_df[\"losses\"].alias(\"team_losses\"))\n",
    "\n",
    "# Join with output\n",
    "output_with_players_and_teams = output.join(players_and_teams,\n",
    "                                            (output[\"team_id\"] == players_and_teams[\"teamid\"]) &\n",
    "                                            (output[\"player_jersey_number\"] == players_and_teams[\"number\"]),\n",
    "                                            \"right\") \\\n",
    "    .select(\n",
    "            output['max_event_id'].alias(\"event_id\"),\n",
    "            output['min_event_timestamp'].alias(\"event_timestamp\"),\n",
    "            players_and_teams['teamid'].alias(\"team_id\"),\n",
    "            players_and_teams['number'].alias(\"player_jersey_number\"),\n",
    "            output['total_shots'],\n",
    "            output['total_goals'],\n",
    "            output['total_team_goals'],\n",
    "            players_and_teams[\"id\"].alias(\"player_id\"),\n",
    "            players_and_teams[\"name\"].alias(\"player_name\"),\n",
    "            players_and_teams[\"team_name\"],\n",
    "            players_and_teams[\"team_conference\"],\n",
    "            players_and_teams[\"team_wins\"],\n",
    "            players_and_teams[\"team_losses\"])\n",
    "\n",
    "# Fill null values with max values for event_id and event_timestamp\n",
    "output_with_players_and_teams = output_with_players_and_teams \\\n",
    "    .fillna(max_event_info[\"max_event_id\"], subset=[\"event_id\"]) \\\n",
    "    .fillna(max_event_info[\"min_event_timestamp\"], subset=[\"event_timestamp\"])\n",
    "\n",
    "# Fill null values with 0 for shots and goals\n",
    "output_with_players_and_teams = output_with_players_and_teams.fillna(0, subset=[\"total_shots\", \"total_goals\"])\n",
    "\n",
    "# Calculate total team goals\n",
    "team_goals_sum = output_with_players_and_teams.groupBy(\"team_id\").agg(F.sum(\"total_goals\").alias(\"sum_total_goals\"))\n",
    "\n",
    "# Fill null values with the sum of total_goals for each team_id\n",
    "output_with_players_and_teams = output_with_players_and_teams.join(\n",
    "    team_goals_sum, \"team_id\", \"left\"\n",
    ").withColumn(\n",
    "    \"total_team_goals\", F.coalesce(\"total_team_goals\", \"sum_total_goals\")\n",
    ").drop(\"sum_total_goals\")  # Drop the redundant total_goals and sum_total_goals columns\n",
    "\n",
    "# Show the final result\n",
    "output_with_players_and_teams.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f121bc-3e03-4e7f-807c-f20e6b9f6c49",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0f960a-0392-4d22-9beb-ee416373a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "\n",
    "from pyspark.sql.functions import col, when, round\n",
    "\n",
    "box_score_data = output_with_players_and_teams\n",
    "\n",
    "# Create a pct column\n",
    "box_score_data = box_score_data.withColumn(\"pct\", \n",
    "                       when(col(\"total_shots\") != 0, round(col(\"total_goals\") / col(\"total_shots\"), 2))\n",
    "                       .otherwise(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3581e98d-5b97-46fe-b851-d653e8759d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+--------------------+-----------+-----------+----------------+---------+-----------+-------------+---------------+---------+-----------+----+-------+\n",
      "|team_id|event_id|event_timestamp|player_jersey_number|total_shots|total_goals|total_team_goals|player_id|player_name|    team_name|team_conference|team_wins|team_losses| pct| status|\n",
      "+-------+--------+---------------+--------------------+-----------+-----------+----------------+---------+-----------+-------------+---------------+---------+-----------+----+-------+\n",
      "|    101|      70|          00:00|                   6|          4|          2|              14|        1|        sam|     syracuse|            acc|       11|          2| 0.5|winning|\n",
      "|    101|      70|          00:00|                   1|          8|          6|              14|        2|      sarah|     syracuse|            acc|       11|          2|0.75|winning|\n",
      "|    101|      70|          00:00|                   2|          7|          2|              14|        3|      steve|     syracuse|            acc|       11|          2|0.29|winning|\n",
      "|    101|      70|          00:00|                  13|          7|          1|              14|        4|      stone|     syracuse|            acc|       11|          2|0.14|winning|\n",
      "|    101|      70|          00:00|                  17|          2|          0|              14|        5|       sean|     syracuse|            acc|       11|          2| 0.0|winning|\n",
      "|    101|      70|          00:00|                   8|          4|          0|              14|        6|        sly|     syracuse|            acc|       11|          2| 0.0|winning|\n",
      "|    101|      70|          00:00|                   9|          5|          0|              14|        7|        sol|     syracuse|            acc|       11|          2| 0.0|winning|\n",
      "|    101|      70|          00:00|                   4|          5|          1|              14|        8|      shree|     syracuse|            acc|       11|          2| 0.2|winning|\n",
      "|    101|      70|          00:00|                  15|          3|          1|              14|        9|     shelly|     syracuse|            acc|       11|          2|0.33|winning|\n",
      "|    101|      70|          00:00|                  10|          3|          1|              14|       10|      swede|     syracuse|            acc|       11|          2|0.33|winning|\n",
      "|    205|      70|          00:00|                   1|          3|          3|               9|       11|      jimmy|johns hopkins|          big10|        9|          4| 1.0| losing|\n",
      "|    205|      70|          00:00|                   9|          4|          0|               9|       12|      julie|johns hopkins|          big10|        9|          4| 0.0| losing|\n",
      "|    205|      70|          00:00|                   2|          3|          1|               9|       13|      james|johns hopkins|          big10|        9|          4|0.33| losing|\n",
      "|    205|      70|          00:00|                  15|          2|          2|               9|       14|       jane|johns hopkins|          big10|        9|          4| 1.0| losing|\n",
      "|    205|      70|          00:00|                  16|          1|          0|               9|       15|      jimmy|johns hopkins|          big10|        9|          4| 0.0| losing|\n",
      "|    205|      70|          00:00|                   8|          2|          1|               9|       16|      julie|johns hopkins|          big10|        9|          4| 0.5| losing|\n",
      "|    205|      70|          00:00|                  17|          3|          1|               9|       17|      james|johns hopkins|          big10|        9|          4|0.33| losing|\n",
      "|    205|      70|          00:00|                   3|          1|          0|               9|       18|       jane|johns hopkins|          big10|        9|          4| 0.0| losing|\n",
      "|    205|      70|          00:00|                   5|          2|          1|               9|       19|      jimmy|johns hopkins|          big10|        9|          4| 0.5| losing|\n",
      "|    205|      70|          00:00|                  22|          1|          0|               9|       20|      julie|johns hopkins|          big10|        9|          4| 0.0| losing|\n",
      "+-------+--------+---------------+--------------------+-----------+-----------+----------------+---------+-----------+-------------+---------------+---------+-----------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round, min, max, lit, when\n",
    "\n",
    "# Get the min and max total_team_goals\n",
    "min_goals = box_score_data.select(min(\"total_team_goals\")).collect()[0][0]\n",
    "max_goals = box_score_data.select(max(\"total_team_goals\")).collect()[0][0]\n",
    "\n",
    "# Determine the count of distinct total_team_goals\n",
    "distinct_goals_count = box_score_data.select(\"total_team_goals\").distinct().count()\n",
    "\n",
    "# Determine the status based on the count of distinct total_team_goals\n",
    "box_score_data = box_score_data.withColumn(\"status\",\n",
    "                       when(lit(distinct_goals_count) == 1, \"tied\")\n",
    "                       .otherwise(\n",
    "                           when(col(\"total_team_goals\") == min_goals, \"losing\")\n",
    "                           .when(col(\"total_team_goals\") == max_goals, \"winning\")\n",
    "                           .otherwise(\"Not tied\")))\n",
    "box_score_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100fb1e4-f3b0-4a8b-bac7-5b7928cdeb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------------+--------------------+\n",
      "|_id|event_timestamp|                home|                away|\n",
      "+---+---------------+--------------------+--------------------+\n",
      "| 70|          00:00|{101, acc, 11, 2,...|{205, big10, 9, 4...|\n",
      "+---+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter home and away teams\n",
    "home_team_data = box_score_data.filter(box_score_data[\"team_id\"] == 101)\n",
    "away_team_data = box_score_data.filter(box_score_data[\"team_id\"] == 205)\n",
    "\n",
    "# Function to format player data\n",
    "def format_player_data(player_data):\n",
    "    return F.collect_list(\n",
    "        F.concat(\n",
    "            F.lit(\"{ id: \"), player_data[\"player_id\"], F.lit(\", name: \"), player_data[\"player_name\"], F.lit(\", shots: \"),\n",
    "            player_data[\"total_shots\"], F.lit(\", goals: \"), player_data[\"total_goals\"], F.lit(\", pct: \"), player_data[\"pct\"], F.lit(\" }\")\n",
    "        )\n",
    "    ).alias(\"players\")\n",
    "\n",
    "# Apply transformations for home team\n",
    "transformed_home_data = home_team_data.groupBy(\"event_id\", \"event_timestamp\").agg(\n",
    "    format_player_data(home_team_data),\n",
    "    F.first(\"team_id\").alias(\"teamid\"),\n",
    "    F.first(\"team_conference\").alias(\"conference\"),\n",
    "    F.first(\"team_wins\").alias(\"wins\"),\n",
    "    F.first(\"team_losses\").alias(\"losses\"),\n",
    "    F.first(\"total_team_goals\").alias(\"score\"),\n",
    "    F.first(\"status\").alias(\"status\")\n",
    ").withColumn(\"score\", F.col(\"score\").cast(\"integer\")) \\\n",
    " .withColumn(\"home\", F.struct(\n",
    "     \"teamid\", \"conference\", \"wins\", \"losses\", \"score\", \"status\", \"players\"\n",
    " )) \\\n",
    " .withColumnRenamed(\"event_id\", \"_id\")\n",
    "\n",
    "# Apply transformations for away team\n",
    "transformed_away_data = away_team_data.groupBy(\"event_id\", \"event_timestamp\").agg(\n",
    "    format_player_data(away_team_data),\n",
    "    F.first(\"team_id\").alias(\"teamid\"),\n",
    "    F.first(\"team_conference\").alias(\"conference\"),\n",
    "    F.first(\"team_wins\").alias(\"wins\"),\n",
    "    F.first(\"team_losses\").alias(\"losses\"),\n",
    "    F.first(\"total_team_goals\").alias(\"score\"),\n",
    "    F.first(\"status\").alias(\"status\")\n",
    ").withColumn(\"score\", F.col(\"score\").cast(\"integer\")) \\\n",
    " .withColumn(\"away\", F.struct(\n",
    "     \"teamid\", \"conference\", \"wins\", \"losses\", \"score\", \"status\", \"players\"\n",
    " )) \\\n",
    " .withColumnRenamed(\"event_id\", \"_id\")\n",
    "\n",
    "# Join home and away data\n",
    "final_data = transformed_home_data.join(transformed_away_data, [\"_id\", \"event_timestamp\"], \"inner\") \\\n",
    "    .select(\"_id\", \"event_timestamp\", \"home\", \"away\")\n",
    "\n",
    "final_data.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86863940-e61f-45bb-8c4c-8c3d64085c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: integer (nullable = true)\n",
      " |-- event_timestamp: string (nullable = false)\n",
      " |-- home: struct (nullable = false)\n",
      " |    |-- teamid: integer (nullable = true)\n",
      " |    |-- conference: string (nullable = true)\n",
      " |    |-- wins: integer (nullable = true)\n",
      " |    |-- losses: integer (nullable = true)\n",
      " |    |-- score: integer (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- players: array (nullable = false)\n",
      " |    |    |-- element: string (containsNull = false)\n",
      " |-- away: struct (nullable = false)\n",
      " |    |-- teamid: integer (nullable = true)\n",
      " |    |-- conference: string (nullable = true)\n",
      " |    |-- wins: integer (nullable = true)\n",
      " |    |-- losses: integer (nullable = true)\n",
      " |    |-- score: integer (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- players: array (nullable = false)\n",
      " |    |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aba58d-1437-486d-86a5-a7adfdc45940",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "\n",
    "# Write to MongoDB\n",
    "final_data.write \\\n",
    "    .format(\"mongo\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"database\", \"sidearm\") \\\n",
    "    .option(\"collection\", \"boxscores\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abfecc6-5461-4249-8169-beb10a1f2838",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba5bd1-3f41-4d03-87ea-ee7726c49320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, round, min, max, lit\n",
    "\n",
    "# Read gamestream data\n",
    "gamestream = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .text(\"s3a://gamestreams/gamestream.txt\") \\\n",
    "    .withColumn('columns', F.split('value', ' ')) \\\n",
    "    .selectExpr(\"columns[0] as event_id\",\n",
    "                \"columns[1] as event_timestamp\",\n",
    "                \"columns[2] as team_id\",\n",
    "                \"columns[3] as player_jersey_number\",\n",
    "                \"columns[4] as shot_result\")\n",
    "\n",
    "# Cast event_id column to integers\n",
    "gamestream = gamestream.withColumn(\"event_id\", F.col(\"event_id\").cast(\"int\"))\n",
    "\n",
    "# Calculate the maximum event_id and corresponding event_timestamp\n",
    "max_event_info = gamestream.selectExpr(\n",
    "    \"max(event_id) as max_event_id\",\n",
    "    \"min(event_timestamp) as min_event_timestamp\"\n",
    ").head()\n",
    "\n",
    "# Aggregations\n",
    "output = gamestream \\\n",
    "    .groupBy(\"team_id\", \"player_jersey_number\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(\"event_id\").alias(\"total_shots\"),\n",
    "        F.sum(F.when(gamestream[\"shot_result\"] == 1, 1).otherwise(0)).alias(\"total_goals\"),\n",
    "        F.min(F.col(\"event_timestamp\")).alias(\"min_event_timestamp\")\n",
    "    )\n",
    "\n",
    "# Fill null values with 0 for total_team_goals\n",
    "output = output.fillna(0, subset=[\"total_goals\"])\n",
    "\n",
    "# Calculate total team goals\n",
    "team_stats = gamestream.groupBy(\"team_id\") \\\n",
    "    .agg(\n",
    "        F.sum(F.when(gamestream[\"shot_result\"] == 1, 1).otherwise(0)).alias(\"total_team_goals\")\n",
    "    )\n",
    "\n",
    "# Join with max_event_info to add event_id and timestamp columns\n",
    "output = output.join(team_stats, \"team_id\", \"inner\")\n",
    "\n",
    "# Add the maximum event ID and timestamp\n",
    "output = output.withColumn(\"max_event_id\", F.lit(max_event_info[\"max_event_id\"])) \\\n",
    "    .withColumn(\"min_event_timestamp\", F.lit(max_event_info[\"min_event_timestamp\"]))\n",
    "\n",
    "# Rearrange columns and rename them\n",
    "output = output.select(\"max_event_id\", \"min_event_timestamp\", \"team_id\", \"player_jersey_number\",\n",
    "                       \"total_shots\", \"total_goals\", \"total_team_goals\")\n",
    "\n",
    "# Read player reference data from MSSQL\n",
    "players_df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "# Read team reference data from MSSQL\n",
    "teams_df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"teams\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .load()\n",
    "\n",
    "# Join players and teams\n",
    "players_and_teams = players_df.join(teams_df, players_df[\"teamid\"] == teams_df[\"id\"], \"left\") \\\n",
    "    .select(players_df[\"*\"], teams_df[\"name\"].alias(\"team_name\"),\n",
    "            teams_df[\"conference\"].alias(\"team_conference\"),\n",
    "            teams_df[\"wins\"].alias(\"team_wins\"),\n",
    "            teams_df[\"losses\"].alias(\"team_losses\"))\n",
    "\n",
    "# Join with output\n",
    "output_with_players_and_teams = output.join(players_and_teams,\n",
    "                                            (output[\"team_id\"] == players_and_teams[\"teamid\"]) &\n",
    "                                            (output[\"player_jersey_number\"] == players_and_teams[\"number\"]),\n",
    "                                            \"right\") \\\n",
    "    .select(\n",
    "            output['max_event_id'].alias(\"event_id\"),\n",
    "            output['min_event_timestamp'].alias(\"event_timestamp\"),\n",
    "            players_and_teams['teamid'].alias(\"team_id\"),\n",
    "            players_and_teams['number'].alias(\"player_jersey_number\"),\n",
    "            output['total_shots'],\n",
    "            output['total_goals'],\n",
    "            output['total_team_goals'],\n",
    "            players_and_teams[\"id\"].alias(\"player_id\"),\n",
    "            players_and_teams[\"name\"].alias(\"player_name\"),\n",
    "            players_and_teams[\"team_name\"],\n",
    "            players_and_teams[\"team_conference\"],\n",
    "            players_and_teams[\"team_wins\"],\n",
    "            players_and_teams[\"team_losses\"])\n",
    "\n",
    "# Fill null values with max values for event_id and event_timestamp\n",
    "output_with_players_and_teams = output_with_players_and_teams \\\n",
    "    .fillna(max_event_info[\"max_event_id\"], subset=[\"event_id\"]) \\\n",
    "    .fillna(max_event_info[\"min_event_timestamp\"], subset=[\"event_timestamp\"])\n",
    "\n",
    "# Fill null values with 0 for shots and goals\n",
    "output_with_players_and_teams = output_with_players_and_teams.fillna(0, subset=[\"total_shots\", \"total_goals\"])\n",
    "\n",
    "# Calculate total team goals\n",
    "team_goals_sum = output_with_players_and_teams.groupBy(\"team_id\").agg(F.sum(\"total_goals\").alias(\"sum_total_goals\"))\n",
    "\n",
    "# Fill null values with the sum of total_goals for each team_id\n",
    "output_with_players_and_teams = output_with_players_and_teams.join(\n",
    "    team_goals_sum, \"team_id\", \"left\"\n",
    ").withColumn(\n",
    "    \"total_team_goals\", F.coalesce(\"total_team_goals\", \"sum_total_goals\")\n",
    ").drop(\"sum_total_goals\")  # Drop the redundant total_goals and sum_total_goals columns\n",
    "\n",
    "\n",
    "\n",
    "# Create a pct column\n",
    "box_score_data = output_with_players_and_teams.withColumn(\"pct\", \n",
    "                       when(col(\"total_shots\") != 0, round(col(\"total_goals\") / col(\"total_shots\"), 2))\n",
    "                       .otherwise(0))\n",
    "\n",
    "# Get the min and max total_team_goals\n",
    "min_goals = box_score_data.select(min(\"total_team_goals\")).collect()[0][0]\n",
    "max_goals = box_score_data.select(max(\"total_team_goals\")).collect()[0][0]\n",
    "\n",
    "# Determine the count of distinct total_team_goals\n",
    "distinct_goals_count = box_score_data.select(\"total_team_goals\").distinct().count()\n",
    "\n",
    "# Determine the status based on the count of distinct total_team_goals\n",
    "box_score_data = box_score_data.withColumn(\"status\",\n",
    "                       when(lit(distinct_goals_count) == 1, \"tied\")\n",
    "                       .otherwise(\n",
    "                           when(col(\"total_team_goals\") == min_goals, \"losing\")\n",
    "                           .when(col(\"total_team_goals\") == max_goals, \"winning\")\n",
    "                           .otherwise(\"Not tied\")))\n",
    "\n",
    "# Filter home and away teams\n",
    "home_team_data = box_score_data.filter(box_score_data[\"team_id\"] == 101)\n",
    "away_team_data = box_score_data.filter(box_score_data[\"team_id\"] == 205)\n",
    "\n",
    "# Function to format player data\n",
    "def format_player_data(player_data):\n",
    "    return F.collect_list(\n",
    "        F.concat(\n",
    "            F.lit(\"{ id: \"), player_data[\"player_id\"], F.lit(\", name: \"), player_data[\"player_name\"], F.lit(\", shots: \"),\n",
    "            player_data[\"total_shots\"], F.lit(\", goals: \"), player_data[\"total_goals\"], F.lit(\", pct: \"), player_data[\"pct\"], F.lit(\" }\")\n",
    "        )\n",
    "    ).alias(\"players\")\n",
    "\n",
    "# Apply transformations for home team\n",
    "transformed_home_data = home_team_data.groupBy(\"event_id\", \"event_timestamp\").agg(\n",
    "    format_player_data(home_team_data),\n",
    "    F.first(\"team_id\").alias(\"teamid\"),\n",
    "    F.first(\"team_conference\").alias(\"conference\"),\n",
    "    F.first(\"team_wins\").alias(\"wins\"),\n",
    "    F.first(\"team_losses\").alias(\"losses\"),\n",
    "    F.first(\"total_team_goals\").alias(\"score\"),\n",
    "    F.first(\"status\").alias(\"status\")\n",
    ").withColumn(\"score\", F.col(\"score\").cast(\"integer\")) \\\n",
    " .withColumn(\"home\", F.struct(\n",
    "     \"teamid\", \"conference\", \"wins\", \"losses\", \"score\", \"status\", \"players\"\n",
    " )) \\\n",
    " .withColumnRenamed(\"event_id\", \"_id\")\n",
    "\n",
    "# Apply transformations for away team\n",
    "transformed_away_data = away_team_data.groupBy(\"event_id\", \"event_timestamp\").agg(\n",
    "    format_player_data(away_team_data),\n",
    "    F.first(\"team_id\").alias(\"teamid\"),\n",
    "    F.first(\"team_conference\").alias(\"conference\"),\n",
    "    F.first(\"team_wins\").alias(\"wins\"),\n",
    "    F.first(\"team_losses\").alias(\"losses\"),\n",
    "    F.first(\"total_team_goals\").alias(\"score\"),\n",
    "    F.first(\"status\").alias(\"status\")\n",
    ").withColumn(\"score\", F.col(\"score\").cast(\"integer\")) \\\n",
    " .withColumn(\"away\", F.struct(\n",
    "     \"teamid\", \"conference\", \"wins\", \"losses\", \"score\", \"status\", \"players\"\n",
    " )) \\\n",
    " .withColumnRenamed(\"event_id\", \"_id\")\n",
    "\n",
    "# Join home and away data\n",
    "final_data = transformed_home_data.join(transformed_away_data, [\"_id\", \"event_timestamp\"], \"inner\") \\\n",
    "    .select(\"_id\", \"event_timestamp\", \"home\", \"away\")\n",
    "\n",
    "\n",
    "\n",
    "# Write to MongoDB\n",
    "final_data.write \\\n",
    "    .format(\"mongo\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"database\", \"sidearm\") \\\n",
    "    .option(\"collection\", \"boxscores\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d433381-aace-4c3b-a03d-71d7a2c9ad94",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7816058a-fdc3-429c-b2be-1832e63dd1f6",
   "metadata": {},
   "source": [
    "# Q10\n",
    "\n",
    "SELECT * \n",
    "FROM mongo.sidearm.boxscores\n",
    "ORDER BY event_timestamp\n",
    "LIMIT 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40b904-d40d-4bca-8c6b-09d2050f38b6",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "039708a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 356:=============================================>         (62 + 1) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+----+------+\n",
      "| id|         name|conference|wins|losses|\n",
      "+---+-------------+----------+----+------+\n",
      "|101|     syracuse|       acc|  12|     2|\n",
      "|205|johns hopkins|     big10|   9|     5|\n",
      "+---+-------------+----------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q11\n",
    "\n",
    "# Join teams_df with box_score_data on team_id\n",
    "joined_df = teams_df.join(box_score_data, teams_df[\"id\"] == box_score_data[\"team_id\"], \"inner\")\n",
    "\n",
    "# Update the wins and losses columns based on conditions\n",
    "teams_updated_df = joined_df.withColumn(\"wins\", \n",
    "    when(((joined_df[\"id\"] == 101) | \n",
    "          (joined_df[\"id\"] == 205)) & (joined_df[\"status\"] == \"winning\") & \n",
    "         (box_score_data[\"event_timestamp\"] == \"00:00\"), joined_df[\"wins\"] + 1)\n",
    "    .otherwise(joined_df[\"wins\"])\n",
    ")\n",
    "\n",
    "teams_updated_df = teams_updated_df.withColumn(\"losses\", \n",
    "    when(((teams_updated_df[\"id\"] == 101) \n",
    "          | (teams_updated_df[\"id\"] == 205)) & (teams_updated_df[\"status\"] == \"losing\") & \n",
    "         (box_score_data[\"event_timestamp\"] == \"00:00\"), teams_updated_df[\"losses\"] + 1)\n",
    "    .otherwise(teams_updated_df[\"losses\"])\n",
    ")\n",
    "\n",
    "# Selecting necessary columns and dropping duplicates\n",
    "teams_final_df = teams_updated_df.select(teams_df.columns).distinct()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "teams_final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c3d0a-6b61-4ebf-922b-57ad46cdadc8",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "473e48fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q12\n",
    "\n",
    "# Write in mssql teams2 table\n",
    "\n",
    "teams_final_df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"teams2\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9408ffc-d809-4913-b3ac-9fa0b05afc09",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b963d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 369:==================================================>  (191 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+------+\n",
      "| id|  name|number|shots|goals|teamid|\n",
      "+---+------+------+-----+-----+------+\n",
      "|  1|   sam|     6|   60|   25|   101|\n",
      "|  2| sarah|     1|   93|   40|   101|\n",
      "|  3| steve|     2|   67|   22|   101|\n",
      "|  4| stone|    13|   40|   11|   101|\n",
      "|  5|  sean|    17|   28|    9|   101|\n",
      "|  6|   sly|     8|   82|   15|   101|\n",
      "|  7|   sol|     9|   57|   20|   101|\n",
      "|  8| shree|     4|   25|    5|   101|\n",
      "|  9|shelly|    15|   13|    3|   101|\n",
      "| 10| swede|    10|   93|   51|   101|\n",
      "| 11| jimmy|     1|  103|   53|   205|\n",
      "| 12| julie|     9|   14|    0|   205|\n",
      "| 13| james|     2|   48|   16|   205|\n",
      "| 14|  jane|    15|   84|   48|   205|\n",
      "| 15| jimmy|    16|   43|   30|   205|\n",
      "| 16| julie|     8|   69|   33|   205|\n",
      "| 17| james|    17|   43|   15|   205|\n",
      "| 18|  jane|     3|   92|   40|   205|\n",
      "| 19| jimmy|     5|   80|   23|   205|\n",
      "| 20| julie|    22|   84|   19|   205|\n",
      "+---+------+------+-----+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q13\n",
    "\n",
    "# Join players_df with box_score_data on player_id\n",
    "joined_df = players_df.join(box_score_data, players_df[\"id\"] == box_score_data[\"player_id\"], \"inner\")\n",
    "\n",
    "# Update the shots and goals columns based on conditions\n",
    "players_updated_df = joined_df.withColumn(\"shots\", \n",
    "    when(box_score_data[\"event_timestamp\"] == \"00:00\",\n",
    "         when(joined_df[\"id\"] == box_score_data[\"player_id\"], joined_df[\"shots\"] + box_score_data[\"total_shots\"])\n",
    "         .otherwise(joined_df[\"shots\"])\n",
    "    )\n",
    "    .otherwise(joined_df[\"shots\"])\n",
    ")\n",
    "\n",
    "players_updated_df = players_updated_df.withColumn(\"goals\", \n",
    "    when(box_score_data[\"event_timestamp\"] == \"00:00\",\n",
    "         when(joined_df[\"id\"] == box_score_data[\"player_id\"], joined_df[\"goals\"] + box_score_data[\"total_goals\"])\n",
    "         .otherwise(joined_df[\"goals\"])\n",
    "    )\n",
    "    .otherwise(joined_df[\"goals\"])\n",
    ")\n",
    "\n",
    "# Select necessary columns and drop duplicates\n",
    "players_final_df = players_updated_df.select(players_df.columns).distinct()\n",
    "\n",
    "# Order by team_id and player_id\n",
    "players_final_df = players_final_df.orderBy(\"team_id\", \"id\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "players_final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb78f13-ed59-4b98-beb5-3fee0b012052",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a048e59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q14\n",
    "\n",
    "# Write in mssql players2 table\n",
    "\n",
    "players_final_df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", mssql_url) \\\n",
    "    .option(\"dbtable\", \"players2\") \\\n",
    "    .option(\"user\", mssql_user) \\\n",
    "    .option(\"password\", mssql_pw) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e7f96-d4b5-42f0-8d34-acb805fa9b3d",
   "metadata": {},
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a36b5ba5-e5e0-4384-83db-fe9ca0397e1d",
   "metadata": {},
   "source": [
    "# Q15\n",
    "\n",
    "SELECT\n",
    "    t.name AS team_name,\n",
    "    t.wins AS team_wins,\n",
    "    t.losses AS team_losses,\n",
    "    p.name AS player_name,\n",
    "    p.shots AS player_shots,\n",
    "    p.goals AS player_goals\n",
    "FROM\n",
    "    teams2 t\n",
    "JOIN\n",
    "    players2 p ON t.id = p.teamid;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
